<!doctype html>
<html lang="en" class="no-js">

<head>
  <meta charset="utf-8">

  <!-- begin SEO -->
  <title>Jiaying Lin - Homepage</title>


  <meta property="og:locale" content="en">
  <meta property="og:site_name" content="Jiaying Lin">
  <meta property="og:title" content="Jiaying Lin">


  <link rel="canonical" href="https://github.com/jfightyr/jolincode.github.io/">
  <meta property="og:url" content="https://github.com/jfightyr/jolincode.github.io/">



  <meta property="og:description" content="Master Candidate &amp; IEEE Student Member">



  <!-- end SEO -->


  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <script>
    document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  </script>

  <!-- For all browsers -->
  <link rel="stylesheet" href="assets/css/main.css">

  <meta http-equiv="cleartype" content="on">

  <head>
    <base target="_blank">
  </head>
  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-jolin.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-jolin.png">
  <link rel="manifest" href="images/site.webmanifest">

  <meta name="msapplication-TileColor" content="#000000">
  <meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
  <meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" href="assets/css/academicons.css" />

  <script
    type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

  <!-- end custom head snippets -->

</head>

<body>


  <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <button>
            <div class="navicon"></div>
          </button>
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a
                href="#about-me">Homepage</a></li>

            <li class="masthead__menu-item"><a href="/#about-me">Biography</a></li>

            <li class="masthead__menu-item"><a href="/#-news">News</a></li>

            <li class="masthead__menu-item"><a href="/#-publications">Main Publications</a></li>

            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>

            <!-- <li class="masthead__menu-item"><a href="/#-visitor-map">visitor ap</a></li> -->

          </ul>
          <ul class="hidden-links hidden"></ul>
        </nav>
      </div>
    </div>
  </div>

  <div id="main" role="main">

    <div class="sidebar sticky">


      <div itemscope itemtype="http://schema.org/Person" class="profile_box">

        <style>
          .author__avatar img {
            width: 130px;
            /* ‰Ω†ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅË∞ÉÊï¥Ëøô‰∏™ÂÄº */
            height: auto;
            /* ËøôÂ∞ÜÁ°Æ‰øùÂõæÁâá‰øùÊåÅÂÖ∂ÂéüÂßãÂÆΩÈ´òÊØî */
          }
        </style>

        <div class="author__avatar">
          <img src="images/jolin.jpg" alt="Jiaying Lin">
        </div>
        <!-- <div class="author__avatar">
          <img src="images/jolin.jpg" class="author__avatar" alt="Jiaying Lin">
        </div> -->

        <div class="author__content">
          <h3 class="author__name">Jiaying Lin</h3>
          <p class="author__bio">Sun Yat-sen University (SYSU)</p>
        </div>

        <div class="author__urls-wrapper">
          <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
          <ul class="author__urls social-icons">

            <!-- <li>
              <div style="white-space: normal; margin-bottom: 1em;">Master Candidate & IEEE Student Member</div>
            </li> -->


            <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Shenzhen, Guangdong, China</li>

            <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Birth:2003.08
            </li>

            <li><a href="mailto:linjy266@mail2.sysu.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i>
                Email</a></li>

            <!-- <li><a href="https://www.researchgate.net/profile/Jiehui-Huang-2"><i class="fab fa-fw fa-researchgate"
                  aria-hidden="true"></i> ResearchGate</a></li> -->

            <li><a href="https://github.com/jfightyr"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a>
            </li>



            <!-- <li><a href="https://scholar.google.com/citations?user=LTtEqGAAAAAJ"><i
                  class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>



            <li><a href="https://orcid.org/0000-0002-3099-2886"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
            -->



          </ul>
          <div class="author__urls_sm">


            <a href="mailto:linjy266@mail2.sysu.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>

            <!-- <a href="https://www.researchgate.net/profile/Jiehui-Huang-2"><i class="fab fa-fw fa-researchgate"
                aria-hidden="true"></i></a> -->

            <a href="https://github.com/jfightyr"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>



            <!-- <a href="https://scholar.google.com/citations?user=LTtEqGAAAAAJ"><i
                class="fas fa-fw fa-graduation-cap"></i></a>



            <a href="https://orcid.org/0000-0002-3099-2886"><i class="ai ai-orcid-square ai-fw"></i></a> -->



          </div>
        </div>
      </div>


    </div>


    <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="headline" content="">
      <div class="page__inner-wrap">
        <section class="page__content" itemprop="text">

          <p><span class="anchor" id="about-me"></span></p>

          <h1 id="Ô∏è-biography">üßç‚Äç‚ôÇÔ∏è Biography</h1>
          <!-- <p>I received a B.S. degree in Automation from <strong><a href="https://www.ncu.edu.cn/">Nanchang
                University</a></strong>, Jiangxi, China, in 2022. I am currently working toward an M.S. degree, advised
            by Xiaodan Liang <a
              href="https://scholar.google.com/citations?view_op=search_authors&amp;mauthors=xiaodan+liang&amp;hl=zh-CN&amp;oi=ao">(Ê¢ÅÂ∞è‰∏π)</a>. -->
          My name is Lin Jiaying, a 2021 undergraduate student from
          <strong><a href="https://ise.sysu.edu.cn/">the School of Intelligent Systems Engineering, Sun Yat-sen
              University</a></strong>, Shenzhen, China.
          <p>During my undergraduate period, I mainly participated in four
            scientific research projects:<strong>"
              Neurodegenerative Disease Diagnostic System", "Multimodal Sentiment Analysis", "Action Recognition
              Based on Skeleton and Point Cloud", and "Spatiotemporal Action Localization"</strong> .</p>
          <!-- I am working in HCPLab, Artificial Intelligence at the School of Intelligent Systems Engineering, <strong><a
              href="https://ise.sysu.edu.cn/">Sun Yat-sen University</a></strong>, Shenzhen, China. I am also lucky to
          have opportunities to collaborate with <a
            href="https://scholar.google.com/citations?user=PUT-52IAAAAJ&amp;hl=zh-CN">Peter. X. Liu</a> (Professor at
          Carleton University, IEEE Fellow), <a href="https://www.ece.pku.edu.cn/info/1053/2659.htm">Calvin Yu-Chian
            Chen</a> (Professor at Peking University).</p> -->

          <!-- , [Chunquan Li](https://teacher.ncu.edu.cn/publish/004904/) (Professor at Nanchang University), [Xuedong He](https://scholar.google.com/citations?user=Jll9EQYAAAAJ&hl=zh-CN) (Tenure-track Assistant Professor at Zhejiang Normal University). -->

          <p>My research interest includes <strong><a
                href="https://scholar.google.com/citations?hl=zh-CN&amp;view_op=search_authors&amp;mauthors=label:computer_vision_and_machine_learning">Computer
                Vision and Machine Learning</a></strong>, Multimodal, Action Recognition and Computer Assisted
            Healthcare. I have published 5 papers at
            the AI Conferences and Journals.</p>

          <h1 id="-news">üî• News</h1>
          <ul>
            <li><em>2024.4</em>: ¬†üéâ 2024 ICME Grand Challenge Multi-Modal Video Reasoning and Analyzing Competition
              (MMVRAC) Track #1, Track #4 Champion Solution;
              Track #10 Top3 Solution.</li>
            <li><em>2024.4</em>: ¬† One paper submitted to ACMMM 2024.
            </li>
            <li><em>2021-2022,2022-2023</em>: ¬†üéâ Received the first-class undergraduate scholarship from Sun Yat sen University.
            </li>
            <li><em>2021-2022</em>: ¬†üéâ Obtain the National Scholarship from the Sun Yat-sen University.</li>
          </ul>

          <!-- <h1 id="-internships">üíª Internships</h1>
          <ul>
            <li><em>2023.10 - 2024.02</em>, <a href="https://research.lenovo.com/webapp/view/researchField.html">Lenovo,
                Research Institute</a>, Shenzhen.</li>
            <li><em>2023.07 - 2023.10</em>, <a href="https://www.sensetime.com/cn">SenseTime, Research Institute</a>,
              Shenzhen.</li>
            <li><em>2022.02 - 2022.05</em>, <a href="https://www.wesure.cn/">Wesure, Recommended algorithm group</a>
              (Owned by Tencent), Shenzhen.</li>
          </ul> -->

          <h1 id="-publications">üìù Main Publications</h1>

          <div class="paper-box">
            <div class="paper-box-image">
              <div>
                <div class="badge">ICMEW,2024</div><img src="images/publications/2024ICMEW_SFMVIT.png" alt="sym"
                  width="100%" />
              </div>
            </div>
            <div class="paper-box-text">
              <p><strong>Spatiotemporal Action Localization</strong></p>
              <p><a href="http://arxiv.org/abs/2404.16609">SFMViT
                  : SlowFast Meet ViT in Chaotic World</a></p>

              <p><strong>Jiaying Lin</strong>, Jiajun Wen, Mengyuan Liu*, Yue Li, Jinfu Liu and Baiqiao Yin </p>

              <p><a href="https://github.com/jfightyr/SlowFast-Meet-ViT"><strong>Project</strong></a> <strong><span
                    class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p>

              <ul>
                <li>We have introduced the dual-stream spatiotemporal feature modeling network SFMViT, which
                  integrates
                  SlowFast's ability
                  to capture temporal features with Vision Transformer's capability in complex scene spatiotemporal
                  modeling. This fusion
                  enhances overall spatiotemporal modeling capabilities in complex scenes.</li>
                <li>We introduce a Confidence Pruning Strategy to find the most suitable upper anchor counts of the
                  instances, which can be
                  used to prune and filter out the optimal anchors while taking into account the efficiency and
                  performance, increasing
                  the mAP by nearly 2%</li>
                <li>We achieve SOTA performances with significant margins on the Chaotic World datasets. At the time
                  of
                  submission, our
                  method ranks first on the 2024 MMVRAC leaderboard.</li>
              </ul>

            </div>
          </div>
          <div class="paper-box">
            <div class="paper-box-image">
              <div>
                <div class="badge">Knowledge-Based System,2024</div><img src="images/publications/2024KBS_TMBL.png"
                  alt="sym" width="100%" />
              </div>
            </div>

            <div class="paper-box-text">
              <p><strong>Multimodal Sentiment Analysis</strong></p>
              <p><a href="https://doi.org/10.1016/j.knosys.2023.111346">TMBL: Transformer-based
                  multimodal binding
                  learning model for multimodal sentiment analysis</a> <br />
                Jiehui Huang, Jun Zhou, Zhenchao Tang, <strong>Jiaying Lin</strong> , and Calvin Yu-Chian Chen*
              </p>

              <p><a href="https://github.com/JackAILab/TMBL"><strong>Project</strong></a> <strong><span
                    class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p>

              <ul>
                <li>Considering that existing multi-modal fusion systems rarely consider fine-grained word-level
                  interactions, we redesigned the Transformer structure, effectively improving the ACC index by 6%.</li>
                <li>In order to solve the problem of modal heterogeneity caused by multi-modal feature fusion, inspired
                  by CLIP, a cross-model binding mechanism was designed for each modality to more effectively fuse modal
                  features.</li>
                <li>Aiming at the modal aliasing problem caused by the difficulty in distinguishing modal features, CLS
                  and PositionEmbedding information are designed to effectively distinguish modal space and semantic
                  relationships.</li>
              </ul>

            </div>
          </div>


          <div class="paper-box">
            <div class="paper-box-image">
              <div>
                <div class="badge">IEEE Transactions on Instrumentation and Measurement,2024</div><img
                  src="images/publications/2024TIM_HD.png" alt="sym" width="100%" />
              </div>
            </div>

            <div class="paper-box-text">
              <p><strong>Neurodegenerative Disease Diagnostic System</strong></p>
              <p><a href="https://github.com/JackAILab/RAJNet">A Novel End-to-end 3D-Residual and Attention
                  Enhancement Joint Few-Shot Learning Model for Huntington Clinical
                  Assessment</a> (major revision) <br />
                Jiehui Huang, Lishan Lin,<strong>Jiaying Lin</strong>, Jun Zhou, Baiqiao Yin, Jean-Marc Burgunder, Zhong
                Pei and Calvin Yu-Chian Chen*
              </p>

              <p><a href="https://github.com/JackAILab/RAJNet"><strong>Project</strong></a> <strong><span
                    class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p>

              <ul>
                <li>We developed an effective end-to-end high-definition diagnosis model based on posture videos.
                  Compared with using gait
                  or biomarker data, posture video-based data is more convenient and can provide reliable diagnostic
                  results.</li>
                <li>We integrated Spatial-Temporal Excitation attention and Spatial and Channel attention modules into
                  the 3D residual
                  network to improve the feature extraction capability. Compared with other machine learning methods,
                  the performance of
                  this structure is more stable and excellent.</li>
                <li>We designed a frame difference algorithm to improve the video data through data enhancement. This
                  new method can
                  overcome issues related to small sample and improve outcome in this few-shot learning task, thereby
                  providing dependable
                  and outstanding diagnostic outcomes. Compared with other Huntington evaluation models, experiments
                  confirm that our
                  model can achieve better recall and accuracy.</li>
              </ul>

            </div>
          </div>


          <div class="paper-box">
            <div class="paper-box-image">
              <div>
                <div class="badge">ACM Multimedia (ACM MM), 2024</div><img
                  src="images/publications/2024ACMMM_ske2po.png" alt="sym" width="100%" />
              </div>
            </div>

            <div class="paper-box-text">
              <p><strong>Action Recognition Based on Skeleton and Point Cloud</strong></p>
              <p><a href="https://yyyybq.github.io/Skeleton2Point.github.io/">Skeleton2Point:
                  Recognizing Skeleton-Based Actions As Point Clouds</a> (in review)
                <br />
                Baiqiao Yin, <strong>Jiaying Lin</strong>, Jiajun Wen, Yue Li, Jinfu Liu and Mengyuan Liu*
              </p>

              <p><a href="https://github.com/yyyybq/Skeleton2Point"><strong>Project</strong></a> <strong><span
                    class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p>

              <ul>
                <li>To our best knowledge, we are the first to regard skeleton joints as point clouds via incorporating
                  the position
                  information of skeletons into point cloud methods, demonstrating the validity of modeling position
                  relationships with 3D
                  coordinates.</li>
                <li>We devise a novel information transformation module (ITM) to merge the original time and series
                  information and the
                  joint coordinates information. We also propose a Cluster-Dispatch-based interaction module (CDI) to
                  focus on overall
                  movement trends.</li>
                <li>We conduct extensive experiments on NTU-RGB+D 60 and NTU-RGB+D 120 datasets to compare our proposed
                  method with the
                  state-of-the-art models in the joint stream and multi-streams. Experimental results demonstrate the
                  significant
                  improvement of our method. In the most challenging NTU120\_XSub\&XSet, our method achieves sota by a
                  large margin.</li>
              </ul>

            </div>
          </div>

          <!-- <div class="paper-box">
            <div class="paper-box-image">
              <div>
                <div class="badge">Computers In Biology And Medicine,2024</div><img
                  src="images/publications/2024CIBM_PD.png" alt="sym" width="100%" />
              </div>
            </div>

            <div class="paper-box-text">
              <p><strong>Neurodegenerative Disease Diagnostic System</strong></p>
              <p><a href="https://doi.org/10.1016/j.knosys.2023.111346">Parkinson's severity diagnosis explainable model
                  based on 3D multi-head attention residual network</a> <br />
                Jiehui Huang, Linsan Lin, Fengcheng Yu, Xuedong He, Wenhui Song, <strong>Jiaying Lin</strong>, Zhenchao
                Tang ,Calvin Yu-Chian Chen* ,
                et. al
              </p>

              <p><a href="https://github.com/JackAILab/MARNet"><strong>Project</strong></a> <strong><span
                    class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p>

              <ul>
                <li>We propose an effective facial video-based end-to-end PD severity diagnosis deep learning model,
                  which can extract
                  features from PD patient videos and give robust and interpretable diagnosis results.</li>
                <li>We effectively integrate channel and spatial attention mechanisms into a 3D neural network model,
                  which ensures that the
                  model can learn non-redundant features. In addition, the effective embedding of LSTM and ResNet
                  further strengthens the
                  robustness of the model.</li>
                <li>We effectively judge the severity of PD disease through various facial expressions of PD. In
                  addition, we also designed
                  a series of interpretability experiments to provide further explanations for our video analysis model,
                  which is of great
                  significance for assisting doctors in diagnosis.</li>
                <li>Our experimental results show that our lightweight model achieves competitive performance while
                  other deep learning
                  models have suboptimal performance. We also performed a correlation analysis between our diagnostic
                  results and clinical
                  data, and experimental results further confirmed the reliability of our results.</li>
              </ul>

            </div> -->
      </div>

      <!-- <div class="paper-box">
            <div class="paper-box-image">
              <div>
                <div class="badge">ACMMM 2024 in review</div><img src="images/publications/2024ACMMM_ske2po.png"
                  alt="sym" width="100%" />
              </div>
            </div> -->

      <!-- <div class="paper-box-text">
              <p><strong>Pose Estimation</strong> </p>
              <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S092523122301189X">Skeleton2Point:
                  Recognizing Skeleton-Based Actions As Point Clouds</a>
                <br />
                Baiqiao Yin, <strong>Jiaying Lin</strong>, Jiajun Wen, Yue Li, Jinfu Liu and Mengyuan Liu*
              </p> -->

      <!-- <p><a href="https://github.com/jfightyr/DTPNet/tree/main"><strong>Project</strong></a> <strong><span
                    class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p> -->

      <!-- <ul>
                <li>To our best knowledge, we are the first to regard skeleton joints as point clouds via incorporating
                  the position
                  information of skeletons into point cloud methods, demonstrating the validity of modeling position
                  relationships with 3D
                  coordinates.</li>
                <li> We devise a novel information transformation module (ITM) to merge the original time and series
                  information and the
                  joint coordinates information. We also propose a Cluster-Dispatch-based interaction module (CDI) to
                  focus on overall
                  movement trends.</li>
                <li>We conduct extensive experiments on NTU-RGB+D 60 and NTU-RGB+D 120 datasets to compare our proposed
                  method with the
                  state-of-the-art models in the joint stream and multi-streams. Experimental results demonstrate the
                  significant
                  improvement of our method. In the most challenging NTU120_XSub\XSet, our method achieves sota by a
                  large margin.</li> -->
      <!-- <li>The progressive learning strategy assists in learning more multi-scale features and achieves SOTA
                  performance on data sets such as SPA-Data, RainDrop, RID, and Rain100.</li> -->
      <!-- </ul>
            </div>
          </div> -->



      <ul>

        <!-- <li><code class="language-plaintext highlighter-rouge">ACMMM 2024 (in review)</code>
          <a href="https://yyyybq.github.io/Skeleton2Point.github.io/">
            Skeleton2Point: Recognizing Skeleton-Based Actions As Point Clouds
          </a>
          ,Baiqiao Yin, <strong>Jiaying Lin</strong>, Jiajun Wen, Yue Li, Jinfu Liu and Mengyuan Liu*
        </li> -->

        <li><code class="language-plaintext highlighter-rouge">Computers In Biology And Medicine,2024</code>
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S001048252400043X">Parkinson's severity
            diagnosis explainable model
            based on 3D multi-head attention residual network</a>
          , Jiehui Huang, Linsan Lin, Fengcheng Yu, Xuedong He, Wenhui Song, <strong>Jiaying Lin</strong>,
          Zhenchao
          Tang ,Calvin Yu-Chian Chen* ,
          et. al

          <!-- <a href="https://github.com/JackAILab/RAJNet">
                  A Novel End-to-end 3D-Residual and
                  Attention
                  Enhancement Joint Few-Shot Learning Model for
                  Huntington Clinical Assessment
                </a>
                ,Jiehui Huang, Lishan Lin,<strong>Jiaying Lin</strong>, Jun Zhou, Baiqiao Yin, Jean-Marc Burgunder,
                Zhong
                Pei and Calvin Yu-Chian Chen* -->
        </li>


        <li><code class="language-plaintext highlighter-rouge">ICMEW 2024</code>
          <a href="https://arxiv.org/abs/2404.15719">
            HDBN: A Novel Hybrid Dual-branch Network for Robust Skeleton-based Action Recognition
          </a>
          ,Jinfu Liu, Baiqiao Yin,<strong>Jiaying Lin</strong>, Jiajun Wen,
          Yue Li and Mengyuan Liu*.
        </li>

        <li><code class="language-plaintext highlighter-rouge">ICMEW 2024</code>
          <a href="https://arxiv.org/abs/2404.19615">
            SemiPL: A Semi-supervised Method for Event Sound Source Localization
          </a>
          ,Yue Li, Jinfu Liu,Baiqiao Yin, Jiajun Wen, <strong>Jiaying Lin</strong>,
          and Mengyuan Liu*.
        </li>

        <!-- <li><code class="language-plaintext highlighter-rouge">Applied Intelligence 2022</code> <a
                href="https://link.springer.com/article/10.1007/s10489-021-02864-8">A novel decomposition-based ensemble
                model for short-term load forecasting using hybrid artificial neural networks</a>, Zhiyuan Liao,
              <strong>Jiaying Lin</strong>, Yuxin Cheng, Chunquan Li, Peter. X. Liu*
            </li>
            <li><code
                class="language-plaintext highlighter-rouge">IET Generation, Transmission &amp; Distribution 2021</code>
              <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/gtd2.12265">A decomposition‚Äêbased
                multi‚Äêtime dimension long short‚Äêterm memory model for short‚Äêterm electric load forecasting</a>, Jinglin
              Liu, <strong>Jiaying Lin</strong>, Zhiwang Zhou, Chunquan Li, Zhiyuan Liao, and Peter. X. Liu*
            </li> -->
      </ul>

      <h1 id="-honors-and-awards">üéñ Honors and Awards</h1>
      <ul>
        <li><em>2024.4</em> ICME Grand Challenge Multi-Modal Video Reasoning and Analyzing Competition
          (MMVRAC) Track #1: Spatiotemporal Action
          Localization, Track #4: Sound Source Localization <strong>Champion Solutions</strong>, and Track #10:
          Skeleton-based Action
          Recognition <strong>Top 3 Solutions</strong>
          .</li>
        <li><em>2021-2022,2022-2023</em> <strong>First Prize</strong> of Sun Yat-sen University Outstanding
          Student
          Scholarship</li>
        <li><em>2022-2023</em> Outstanding Student Leader of School of Intelligent Systems Engineering, Sun
          Yat-sen
          University</li>
        <li><em>2023</em> MCM/ICM(Mathematical Contest In Modeling and Interdisciplinary Contest In Modeling)
          <strong>M Award</strong>
        </li>
        <li><em>2021-2022</em> <strong>National Scholarship</strong> of Sun Yat-sen University(Top 1%)</li>
        <li><em>2021-2022</em> Outstanding Communist Youth League Member of Sun Yat-sen University</li>
        <li><em>2022</em> APMCM Asia and Pacific Mathematical Contest in Modeling <strong>National
            First Prize</strong> </li>
        <li><em>2022</em> Fifth place in the finals of the Sun Yat-sen University "Kangle Cup" Traditional Martial
          Arts Championship</li>

      </ul>

      <!-- <h1 id="-educations">üìñ Educations</h1> -->
      <!-- - *2022.06 - (now)*, M.S in Artificial Intelligence, School of Intelligent Systems Engineering, Sun Yat-sen University, Guangdong. 
- *2018.09 - 2022.06*, B.E in Automation, School of Intelligent Systems Engineering, Nanchang University, Jiangxi, Automation.
- *2015.09 - 2018.06*, Jiangxi Linchuan No.1 Middle School, China. -->

      <!-- <div class="school-box"> -->
      <!-- <div><img src='images/Schools/SYSU.jpeg' alt="sym" width="80"></div> -->
      <!-- <div class="school-box-text">
              <p>2022.09 - now, M.S Student.</p>

              <p>Artificial Intelligence, School of Intelligent Systems Engineering(ISE).</p>

              <p>Sun Yat-sen University, Shenzhen.</p>
            </div>
          </div> -->

      <!-- <div class="school-box"> -->
      <!-- <div><img src='images/Schools/NCU.jpg' alt="sym" width="80"></div> -->
      <!-- <div class="school-box-text">
              <p>2018.09 - 2022.06, Undergraduate.</p>

              <p>Automation, School of Intelligent Systems Engineering.</p>

              <p>Nanchang University (NCU), Nanchang.</p>
            </div> -->
      <!-- </div> -->

      <h1 id="Ô∏è-visitor-map">üó∫Ô∏è Visitor Map</h1>
      <script type="text/javascript"
        src="//rf.revolvermaps.com/0/0/6.js?i=54e0ojatafc&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80"
        async="async"></script>


      </section>
  </div>
  </article>
  </div>

  <script src="assets/js/main.min.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', "");
  </script>


  <script>
    $(document).ready(function () {

      var gsDataBaseUrl = 'https://raw.githubusercontent.com/JackAILab/JackAILab.github.io/'

      $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
        var totalCitation = data['citedby']
        document.getElementById('total_cit').innerHTML = totalCitation;
        var citationEles = document.getElementsByClassName('show_paper_citations')
        Array.prototype.forEach.call(citationEles, element => {
          var paperId = element.getAttribute('data')
          var numCitations = data['publications'][paperId]['num_citations']
          element.innerHTML = '| Citations: ' + numCitations;
        });
      });
    })
  </script>


</body>

</html>
